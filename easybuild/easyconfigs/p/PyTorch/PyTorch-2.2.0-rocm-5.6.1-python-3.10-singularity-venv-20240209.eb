# Developed by Kurt Lust and Mihkel Tiks for LUMI
easyblock = 'MakeCp'

local_c_rocm_version = '5.6.1'
local_c_python_mm = '3.10'
local_c_PyTorch_version = '2.2.0'

local_c_DeepSpeed_version = '0.12.3'
local_c_flashattention_version = '2.0.4'
local_c_xformers_version = '0.0.25+8dd471d.d20240209'

local_conda_env = 'pytorch'


name =          'PyTorch'
version =       local_c_PyTorch_version
versionsuffix = f'-rocm-{local_c_rocm_version}-python-{local_c_python_mm}-singularity-venv-20240209'

local_sif =    f'lumi-pytorch-rocm-{local_c_rocm_version}-python-{local_c_python_mm}-pytorch-v2.2.0-dockerhash-f72ddd8ef883.sif'
#local_docker = f'lumi-pytorch-rocm-{local_c_rocm_version}-python-{local_c_python_mm}-pytorch-v2.2.0.docker'

homepage = 'https://pytorch.org/'

whatis = [
    'Description: PyTorch, a machine learning package',
    'Keywords: PyTorch, DeepSpeed, flash-attention'
]

description = f"""
This module provides a container with PyTorch %(version)s on Python {local_c_python_mm}. 
It also contains DeepSpeed {local_c_DeepSpeed_version}, flash-attention {local_c_flashattention_version} and xformers
{local_c_xformers_version}.

The module defines a number of environment variables:
*   SIF and SIFPYTORCH: The full path and name of the Singularity SIF file 
    to use with singularity exec etc.
*   SINGULAIRTY_BINDPATH: Mounts the necessary directories from the system,
    including /users, /project, /scratch and /flash so that you should be
    able to use your regular directories in the container.
*   RUNSCRIPTS and RUNSCRIPTSPYTORCH: The directory with some sample
    runscripts.
    
Note that this container uses a Conda environment internally. When in
the container, the command to activate the container is contained in the
environment variable WITH_CONDA.
"""

docurls = [
    'DeepSpeed web site: https://www.deepspeed.ai/'    
]

toolchain = SYSTEM

sources = [
    {
        'filename':    local_sif,
        'extract_cmd': '/bin/cp -L %s .'
    },
#    {
#        'filename':    local_docker,
#        'extract_cmd': '/bin/cp -L %s .'
#    },
]

skipsteps = ['build']

files_to_copy = [
    ([local_sif],    '.'),
#    ([local_docker], 'share/docker-defs/')    
]

#
# Code for scripts in the bin subdirectory
#

local_bin_start_shell="""
#!/bin/bash -e

# Run application
if [[ -f "/.singularity.d/Singularity" ]] 
then
    # In a singularity container, just in case a user would add this to the path.
    # There is no need here to distinguish between calling with and without arguments.
    eval "\$INIT_CONTAINER" ; exec bash "\$@"
else
    # Not yet in the container
    if [ "\$#" -eq 0 ]
    then
        singularity exec \$SIFPYTORCH bash -c 'eval "\\$INIT_CONTAINER" ; exec bash'
    else
        singularity exec \$SIFPYTORCH bash "\$@"
    fi
fi

"""

#
# Code for scripts in the runscript subdirectory
#

local_runscript_init_conda_venv=f"""
#
# Source this file to initialize both the Conda environment and 
# predefined virtual environment in the container.
#
source /opt/miniconda3/bin/activate {local_conda_env}
source /user-software/venv/venv-{local_conda_env}/bin/activate

"""

local_runscript_python_simple="""
#!/bin/bash -e

# Start conda environment inside the container
eval "\$INIT_CONTAINER"

# Run application
python "\$@"

"""

local_runscript_python_distributed="""
#!/bin/bash -e

# Make sure GPUs are up
if [ \$SLURM_LOCALID -eq 0 ] ; then
    rocm-smi
fi
sleep 2

export MIOPEN_USER_DB_PATH="/tmp/\$(whoami)-miopen-cache-\$SLURM_NODEID"
export MIOPEN_CUSTOM_CACHE_DIR=\$MIOPEN_USER_DB_PATH

# Set MIOpen cache to a temporary folder.
if [ \$SLURM_LOCALID -eq 0 ] ; then
    rm -rf \$MIOPEN_USER_DB_PATH
    mkdir -p \$MIOPEN_USER_DB_PATH
fi
sleep 2

# Report affinity
echo "Rank \$SLURM_PROCID --> \$(taskset -p \$\$)"

# Start conda environment and virtual environment inside the container
eval "\$INIT_CONTAINER"

# Set interfaces to be used by RCCL.
export NCCL_SOCKET_IFNAME=hsn0,hsn1,hsn2,hsn3

# Set environment for the app
export MASTER_ADDR=\$(/runscripts/get-master "\$SLURM_NODELIST")
export MASTER_PORT=29500
export WORLD_SIZE=\$SLURM_NPROCS
export RANK=\$SLURM_PROCID
export ROCR_VISIBLE_DEVICES=\$SLURM_LOCALID

# Run application
python "\$@"

"""

local_runscript_get_master="""
#!/usr/bin/python3
# This is the correct Python path both on LUMI and in the container, but the script
# should really be used in the container.

import argparse
def get_parser():
    parser = argparse.ArgumentParser(description="Extract master node name from Slurm node list",
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument("nodelist", help="Slurm nodelist")
    return parser


if __name__ == '__main__':
    parser = get_parser()
    args = parser.parse_args()

    first_nodelist = args.nodelist.split(',')[0]

    if '[' in first_nodelist:
        a = first_nodelist.split('[')
        first_node = a[0] + a[1].split('-')[0]

    else:
        first_node = first_nodelist

    print(first_node)

"""

#
# Now install does scripts and do further preparations of the container.
#

local_singularity_bind = '/var/spool/slurmd,/opt/cray,/usr/lib64/libcxi.so.1,/usr/lib64/libjansson.so.4,' + \
                         '%(installdir)s/runscripts:/runscripts,' + \
                         '/pfs,/scratch,/projappl,/project,/flash,/appl' + \
                         ',%(installdir)s/user-software:/user-software'

postinstallcmds = [
    # Install the scripts in the bin subdirectory
    'mkdir -p %(installdir)s/bin',
    f'cat >%(installdir)s/bin/start-shell <<EOF {local_bin_start_shell}EOF',
    'chmod a+x %(installdir)s/bin/start-shell',    
    # Install the runscripts
    'mkdir -p %(installdir)s/runscripts',
    f'cat >%(installdir)s/runscripts/init-conda-venv <<EOF {local_runscript_init_conda_venv}EOF',
    'chmod a-x %(installdir)s/runscripts/init-conda-venv',
    f'cat >%(installdir)s/runscripts/conda-python-simple <<EOF {local_runscript_python_simple}EOF',
    'chmod a+x %(installdir)s/runscripts/conda-python-simple',
    f'cat >%(installdir)s/runscripts/conda-python-distributed <<EOF {local_runscript_python_distributed}EOF',
    'chmod a+x %(installdir)s/runscripts/conda-python-distributed',
    f'cat >%(installdir)s/runscripts/get-master <<EOF {local_runscript_get_master}EOF',
    'chmod a+x %(installdir)s/runscripts/get-master',
    # Create the virtual environment and space for other software installations that
    # can then be packaged.
    'mkdir -p %(installdir)s/user-software/venv',
    # For the next command, we don't need all the bind mounts yet, just the user-software one is enough.
    f'singularity exec --bind %(installdir)s/user-software:/user-software %(installdir)s/{local_sif} bash -c \'$WITH_CONDA ; cd /user-software/venv ; python -m venv --system-site-packages venv-{local_conda_env}\'' 
]

sanity_check_paths = {
    # We deliberately don't check for local_sif as the user is allowed to remove that file
    # but may still want to regenerate the module which would then fail in the sanity check.
    #'files': [f'share/docker-defs/{local_docker}'],
    'files': [],
    'dirs':  ['runscripts'],
}

modextravars = {
    # SIF variables currently set by a function via modluafooter.
    #'SIF':                          '%(installdir)s/' + local_sif,
    #'SIFPYTORCH':                   '%(installdir)s/' + local_sif,
    'RUNSCRIPTS':                    '%(installdir)s/runscripts',
    'RUNSCRIPTSPYTORCH':             '%(installdir)s/runscripts',
    'SINGULARITY_BIND':              local_singularity_bind,
    'SINGULARITYENV_WITH_VENV':      f'source /user-software/venv/venv-{local_conda_env}/bin/activate',
    'SINGULARITYENV_INIT_CONTAINER': 'source /runscripts/init-conda-venv',
    'SINGULARITYENV_PREPEND_PATH':   '/runscripts',
}

modluafooter = f"""
-- Call a routine to set the various environment variables.
create_container_vars( '{local_sif}', 'PyTorch', '%(installdir)s' )
"""

moduleclass = 'devel'




